{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "So far we have grouped Machine Learning into Supervised Learning and\n",
    "Unsupervised Learning. There is a third branch of Machine Learning called\n",
    "Reinforcement Learning. It is motivated by the way humans are belived to learn,\n",
    "by **interacting with their environment**.\n",
    "\n",
    "The goal of Reinforcement Learning is to map actions to situations (states) so as to \n",
    "**maximize a numerical reward signal**.\n",
    "\n",
    "https://www.youtube.com/watch?v=kopoLzvh5jY\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "We define an **agent**, that takes **actions**. These actions lead to a **reward** and influence the **state** of the environment.\n",
    "\n",
    "![title](sar.jpg)\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "Reinforcement Learning problems are commonly defined as Markov Decision Processes (MDP) that are defined by a **state space** *S*, an **action space** *A*, a **state transition function** *P*, a **reward function** *r* and a **discount factor** *$\\gamma$*.\n",
    "\n",
    "### State Space *S*\n",
    "\n",
    "$S = \\{s_1, s_2, ..., s_n\\}$\n",
    "\n",
    "All n possible states of the environment.\n",
    "\n",
    "### Action Space *A*\n",
    "\n",
    "$A = \\{a_1, a_2, ..., a_m\\}$\n",
    "\n",
    "All m possible actions of the agent.\n",
    "\n",
    "### State Transition Function *P*\n",
    "\n",
    "$P(s', r|s, a)$\n",
    "\n",
    "The probability distribution of entering state $s'$ and receiving reward *r* after choosing action *a* in state *s*. It defines the dynamics of the MDP.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "$R(a, s)$\n",
    "\n",
    "The reward of the agent for choosing action a in state s.\n",
    "\n",
    "### Discount Factor\n",
    "\n",
    "$\\gamma$\n",
    "\n",
    "The factor with which future rewards are discounted. Usually a discount factor $\\gamma < 1$ is used to indicate that future reward is worth less than current reward.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Find a policy $\\pi(a|s)$ that maximizes the total expected reward $V_\\pi(s) = E[G_t|S_t = s] \\forall s$ where $G_t = R_{t+1} + \\gamma * R_{t+2} ... + \\gamma^{p-1} * R_{t+p}$ is the **return**.\n",
    "\n",
    "#### Return\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma * R_{t+2} ... + \\gamma^{p-1} * R_{t+p}$\n",
    "\n",
    "The return at time t is the cumulated future discounted return.\n",
    "\n",
    "#### Policy\n",
    "\n",
    "$\\pi(a|s)$\n",
    "\n",
    "Is a mapping of a single action to every state of the environment.\n",
    "\n",
    "#### State-Value Function\n",
    "\n",
    "$V_\\pi(s) = E[G_t|S_t=s]$\n",
    "\n",
    "The state-value function for policy $\\pi(a|s)$ is the total expected return from being in state S=s and following the policy $\\pi(a|s)$.\n",
    "\n",
    "#### Action-Value Function\n",
    "\n",
    "$Q_\\pi(s, a) = E[G_t|S_t=s, A_t=a]$\n",
    "\n",
    "The action-value function for policy $\\pi(a|s)$ is the total expected return from being in state S=s, chosing action A=a and thereafter following policy $\\pi(a|s)$.\n",
    "\n",
    "One important property of both value functions is that they are **recursive relationships**. E.g.\n",
    "\n",
    "\n",
    "$Q_\\pi(s, a) = E[G_t|S_t=s, A_t=a] = $\n",
    "\n",
    "$E[R_{t+1} + \\gamma*G_{t+1}|S_t=s, A_t=a] = $\n",
    "\n",
    "$E[R_{t+1}|S_t=s, A_t=a] + E[\\gamma*V_\\pi(s')|S_t+1=s']$\n",
    "\n",
    "This is called the **Bellman equation** and is central to Reinforcement Learning.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We want to find a policy that optimizes the return (sum of discounted future rewards) of the agent.\n",
    "\n",
    "Let's look at an example.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- Mouse can go left, right, up and down\n",
    "- If the mouse finds cheese, either in the upper right corner or in the lower left corner, the environment terminates\n",
    "- The mouse prefers two blocks of cheese over one block of cheese\n",
    "\n",
    "![](mouse_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are state space, action space, state transition function, reward function and discount factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we find the policy?\n",
    "\n",
    "1. Dynamic Programming: Do not learn from the environment and require knowledge of the dynamics and the rewards\n",
    "- Monte Carlo Methods: Learn directly from the environment but only after the final outcome is observed\n",
    "- Temporal Difference Learning: Learn directly from the environment and update estimates from other learned estimates\n",
    "- Policy Gradients: Approximate the value function through a parametrized function (Deep Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "\n",
    "* Credit assignment Problem\n",
    "* Exploration vs. Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement it in practice using OpenAI's Gym\n",
    "* A handy library for learning about RL - https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the cartpole problem\n",
    "#### First we make an environment in which the agent can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We survived 14 steps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.08)\n",
    "    if done:\n",
    "        print(f'We survived {i} steps')\n",
    "        env.reset()\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we implement the agent-environment loop\n",
    "* Start the process by resetting the environment\n",
    "* And return an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01848355,  0.01098999,  0.0157114 , -0.03951064])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_obs\n",
    "#position of cart, velocity of cart, angle of pole, rotation of pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[position of cart, velocity of cart, angle of pole, rotation rate of pole\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by taking an action - in this case a  `step` in a given direction, 0 for left and 1 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.step(0) # move cart left \n",
    "obs, reward, done, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already use the `done` boolean to work out if we can stop the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02195082,  0.01055209,  0.02008294, -0.02985178]), 1.0, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `sample` the `action_space` space to randomly pick an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_step = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `render` the environment to see what our cart is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, but we need to build an RL agent. What next?**\n",
    "\n",
    "First, lets try to build the simplest RL agent:\n",
    "* If the pole is left, move left\n",
    "* If the pole is right, move right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rl(env):\n",
    "    #reset the environment and taking an initial step\n",
    "    obs = env.reset()\n",
    "    \n",
    "     #loop over this process until I die\n",
    "    for i in range(1000):\n",
    "        \n",
    "    #measure: is my pole angled to the left, or the right\n",
    "    #action: if pole is left, move cart left. if pole is right, move right\n",
    "        if obs[2] < 0:\n",
    "            action = 0\n",
    "        elif obs[2] >0:\n",
    "            action = 1\n",
    "        elif obs[2] == 0:\n",
    "            print('omgomgomg were amazing')\n",
    "            break\n",
    "            \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.08) #to make the video play at a normal rate\n",
    "        if done:\n",
    "            print(f'iterations survived: {i}')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark for a dumb rl agent = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived: 34\n"
     ]
    }
   ],
   "source": [
    "simple_rl(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some evolutionary algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = np.random.rand(4) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22711813,  0.90011165,  0.04559917,  0.9696599 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00533671, -0.0396626 , -0.04493139, -0.00146532])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03795839522582525"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(parameters, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, parameters, range_=200, render=False):  \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    \n",
    "    for _ in range(range_):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(0.08)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, parameters, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 episodes required to reach a reward of 400\n"
     ]
    }
   ],
   "source": [
    "re_re = 400\n",
    "bestparams = None  \n",
    "bestreward = 0  \n",
    "for i in range(1000):  \n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters, range_=re_re)\n",
    "    \n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts 200 timesteps\n",
    "        if reward == re_re:\n",
    "            print(f'{i} episodes required to reach a reward of {re_re}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02159105,  0.79203038,  0.7630268 ,  0.7267915 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, bestparams, re_re, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
    "\n",
    "model = DQN(MlpPolicy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 24       |\n",
      "| steps                   | 2375     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 80.8     |\n",
      "| steps                   | 10459    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 143      |\n",
      "| steps                   | 24716    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 128      |\n",
      "| steps                   | 37472    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 118      |\n",
      "| steps                   | 49300    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 47.7     |\n",
      "| steps                   | 54068    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 131      |\n",
      "| steps                   | 67211    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 89.9     |\n",
      "| steps                   | 76202    |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)\n",
    "model.save(\"deepq_cartpole\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = DQN.load(\"deepq_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 episodes\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "i = 0\n",
    "total_rewards = []\n",
    "while not done:\n",
    "    i += 1\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    total_rewards.append(rewards)\n",
    "    time.sleep(0.08)\n",
    "    env.render()\n",
    "print(f'{i} episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
