{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives:\n",
    "* Setting up a scrapy project\n",
    "* Classes & Inheritance\n",
    "* Generators\n",
    "* Xpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a scrapy project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Install the packages\n",
    "\n",
    "First we'll need to `pip install scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: We can create a project using the command:\n",
    "\n",
    "`scrapy startproject <PROJECT>`\n",
    "\n",
    "This will create a default project. Lets to understand the file structure that scrapy creates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "├── <PROJECT>\n",
    "│   ├── __init__.py\n",
    "│   ├── items.py\n",
    "│   ├── middlewares.py\n",
    "│   ├── pipelines.py\n",
    "│   ├── __pycache__\n",
    "│   ├── settings.py\n",
    "│   └── spiders\n",
    "│       ├── __init__.py\n",
    "│       ├── __pycache__\n",
    "│       └── scraper.py\n",
    "└── scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following files aren't core to learning Scrapy basics\n",
    "\n",
    "* scrapy.cfg - the scrapy deployment config file\n",
    "* items.py - temporary storage for data scraped (not necessary)\n",
    "* middleware.py - download customised middleware as required\n",
    "* pipelines.py - create a customsed pipeline for more complex projects\n",
    "* settings.py - customise your project, e.g. api settings, scraping layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We really only care about the code in the spiders folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes & inheritance\n",
    "* Classes are a design pattern core to Object Oriented Programming Languages like Python. It allows us to wrap data and functions together in a reusable code package\n",
    "* Inheritance is a technique for using funcitonality written in other classes.\n",
    "* Don't worry too much about these ideas; we can just copy the structure Scrapy gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators\n",
    "* Generators are iterables which are only revealed once piece at a time, whenever the user wants. They are commonly used:\n",
    "    * When you don't know exactly how many items your iterable is going to contain\n",
    "    * When you worry you might have too much data in the iterable to fit into memory\n",
    "* You can find generators in the python wilderness in the following:\n",
    "    * zip() / enumerate() / open() / df.iterrows()\n",
    "    * The list goes on.... looks like we could use a generator for it!\n",
    "* You can create them using the `yield` command. This works like return but in a generative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xpaths\n",
    "* Xpaths are a way of expressing the structure of webpages (technically the DOM) more like a file directory. This should make reading these xpaths a bit intuitive for UNIX users!\n",
    "\n",
    "**3 basic statements**:\n",
    "\n",
    "* `//` = select from the entire document (irrespective of where)\n",
    "* `/` = select from the root node (of the current query)\n",
    "* `@` =  select attributes specifically\n",
    "* `[]` =  select predicates (i,e with conditional statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK! The background is out the way, lets get one of these spiders up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.http import Request\n",
    "import re\n",
    "\n",
    "#inherit from scrapy.Spider\n",
    "class LyricsScraper(scrapy.Spider):\n",
    "    #define the name of the spider which we will call from CLI\n",
    "    name = 'scrape_lyrics'\n",
    "    #set the range of urls which can be scraped over\n",
    "    allowed_domain = ['https://www.metrolyrics.com/']\n",
    "    start_urls = ['https://www.metrolyrics.com/top-artists.html/']\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        #look through all the artists in the artists page\n",
    "        #this returns a list of urls\n",
    "        artists = response.xpath('//a[@class=\"image\"]/@href').getall()\n",
    "        for artist in artists:\n",
    "            yield Request(artist)\n",
    "\n",
    "        #find all their song urls\n",
    "        songs = response.xpath('//td/a/@href').getall()\n",
    "        for song in songs:\n",
    "            yield Request(song)\n",
    "\n",
    "        #connect to the url of each song\n",
    "        #scrape the lyrics from this page\n",
    "        lyrics = response.xpath('//div[@id=\"lyrics-body-text\"]//text()').getall()\n",
    "        lyrics = ' '.join(lyrics)\n",
    "\n",
    "        item = {'lyrics': lyrics}\n",
    "        yield item\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways you can improve the Spider:\n",
    "* Write the results to disk rather than printing\n",
    "* Log the status of the scrape to terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further research:\n",
    "\n",
    "Scrapy documentation: `https://docs.scrapy.org/en/latest/intro/tutorial.html`\n",
    "\n",
    "Spiced scrapy documentation:`Chapter 6.4` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
